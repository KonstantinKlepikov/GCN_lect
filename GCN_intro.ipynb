{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение на графах (задача)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Главная проблема - разыскать такое представление графа, которое отобразит графовую структуру в модель машинного обучения. Решается это так:\n",
    "- графовые стаистики (коэф.кластеризации и т.п. и т.д.)\n",
    "- ядерные методы [Graph Kernels 2010](https://www.jmlr.org/papers/volume11/vishwanathan10a/vishwanathan10a.pdf)\n",
    "- манипуляции с признаками для оценки локальной близости\n",
    "- обучение представлению графа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последняя идея носит название representation learning и позволяет отобразить структурную информацию о графе в виде векторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представление информации в нодах или субграфах в виде векторов - проблема. Оказывается, как правило, данные содержатся в многомерном неевклидовом пространстве. А нам нужны векторы в пространстве, в котором мы можем классифицировать, кластеризировать, предсказывать какую-то вероятность и делать кучу других манипуляций ML. Соответственно **задача - перегнать многомерный вектор признаков в низкоразмерный, размерностью $d$, желательно из $\\mathbb{R}$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![захарьевские против солнцевских](img/01.png)\n",
    "\n",
    "Тот самый Zachary Karate Club social network, слева показана связь между группировской захарьевских, солнцевских и других бандитов, а справа двумерное представление, позволяющее как-то работать с этой сетью осмысленно с точки зрения ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В статье:\n",
    "**[Representation Learning on Graphs: Methods and Applications](https://arxiv.org/abs/1709.05584)**\n",
    "William L. Hamilton\n",
    "Rex Ying\n",
    "Jure Leskovec\n",
    "arXiv:1709.05584v3 \\[cs.SI\\] 10 Apr 2018\n",
    ", если опустить подробности, задача эта описывается так:\n",
    "\n",
    "Если мы имеем граф $G = (V, E)$ и ассоциированные с ним матрицу смежности $A$ и матрицу $X$, содержащую атрибуты нод, так что $X \\in \\mathbb{R}^{m \\times \\vert V \\vert}$, то задачей является получение векторов $z \\in \\mathbb{R}^d$ для каждой ноды, таких, что $d \\ll \\vert V \\vert$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- функция попарной похожести (pairwise similarity function) $S_g : V \\times V \\rightarrow \\mathbb{R}^+$, определенная над графом $G$. Функция измеряет схожесть между нодами.\n",
    "- энкодер, генерирующий эмбеддинги $ENC: V \\rightarrow  \\mathbb{R}^d$\n",
    "- декодер, реконструирующий статистики графа из эмбеддингов $DEC : \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^+$ \\*\n",
    "- функция потерь (специфична для каждой задачи) - оценивает несоответствие между декодированным (оцененным) значением близости $DEC(\\mathbf{z}_i, \\mathbf{z}_j)$ и истинным $S_g(v_i, v_j)$\n",
    "\n",
    "\\* для двух нод в графе $DEC(ENC(v_i), ENC(v_j)) = DEC(\\mathbf{z}_i, \\mathbf{z}_j) \\approx S_g(v_i, v_j)$ декодер восстанавливает \"попарную близость\" нод в графе из эмбеддингов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Энкодеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Самый простой метод** - shallow encoding, для которого мы можем определить энкодер как функцию: $ENC(v_i) = Z v_i$, где $Z \\in \\mathbb{R}^{m \\times \\vert V \\vert}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![шаловливый энкодер](img/02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К подобным методам относятся различные <span class=\"burk\">матричные факторизации</span> (graph factorisation, GraRep, HOPE и т.д.) и RandomWalk модели (DeepWalk, node2wec).\n",
    "\n",
    "![шаловливые энкодеры - методы](img/03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проблемы подхода**\n",
    "\n",
    "- никакие параметры не используются совместно в энкодере \n",
    "- <span class=\"burk\">число параметров растет как функция от количества нод</span> $O(|V|)$\n",
    "- подход не учитывает атрибуты нод, которые в большинстве случаев содержат важную информацию о графе\n",
    "- подход генерирует эмбеддинги только для данных, которые есть среди обучаемых. Модель проблематично обобщить на данные, которые модель никогда не видела"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized encoder-decoder architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1) Neighborhood autoencoder methods** (<span class=\"burk\">автоэнкодеры окрестностей</span>)\n",
    "\n",
    "- DNGR (deep neural graph representation)\n",
    "- SDNE (structural deep network embeddings)\n",
    "\n",
    "Каждая нода $v_i$ ассоциируется с высокаразмерным \"вектором близости\" $S_i \\in \\mathbb{R}^{\\vert V \\vert}$, содержащем информацию о близости ноды кнодам в окресности в графе. Затем данный вектор сжимается до нужной размерности.\n",
    "\n",
    "![Generalized encoder-decoder architectures](img/06.png)\n",
    "\n",
    "Проблема такого подхода в том, что он невероятно дорогой. К тому же метод статичный и плохо работает с изменяющимися графами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2) Neighborhood aggregation and convolutional encoders**\n",
    "\n",
    "Этот метод позволяет <span class=\"burk\">агрегировать</span> «месседжи» от соседей ноды, которые, в свою очередь, базируются на «месседжах», агрегированных по соседям соседей и т.д. Иногда такие модели называют сверточными энкодерами из-за схожести их архитектуры со свертками.\n",
    "\n",
    "![Neighborhood aggregation and convolutional encoders](img/04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сверточные энкодеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Алгоритм](img/05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Алгоритм строит эмбеддинги для нод рекурсивно \n",
    "- Вначале энкодер инициализируется атрибутами нод \n",
    "- Затем на каждой итерации агрегируются эмбеддинги соседей \n",
    "- Далее каждая нода получает новый эмбеддинг, скомбинированный из ее собственного эмбеддинга и агрегированного вектора.\n",
    "- Затем все это отправляется в полносвязный слой и процесс повторяется K раз.\n",
    "\n",
    "Эмбеддинг каждой ноды обогощается данными своих соседей, при этом размерность векторов остается постоянной во время всей процедуры. В результате мы получаем векторное представление графа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот подход эксплуатируют:\n",
    "- GNN (graph neural network)\n",
    "- GCN (graph convolutional networks) \\*\n",
    "- column networks\n",
    "- GraphSAGE\n",
    "\n",
    "\\* В лекциях cs224w понятия GNN и GCN сильно смешаны. Исторически сложилось, что GNN (graph neural network) - это отдельная история, развиваввшася самостоятельно и раньше. GNN - обобщает разные модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разные подтипы модели определяются разными агрегирующей функцией (стр.4 алгоритма) и комбинирующей вектора функцией (стр.5)\n",
    "\n",
    "### GCN\n",
    "- агрегирующая функция - поэлементное среднее\n",
    "- комбинируем взвешенной суммой \n",
    "\n",
    "### column network\n",
    "- тоже самое, что и в GCN, только на выходе цикла стоит функция интерполяции, позволяющая сохранять локальную информацию в процессе итерации) [см. статью](https://arxiv.org/abs/1609.04508)\n",
    "\n",
    "### GraphSAGE\n",
    "- агрегирует среднее, макспуллинг или LSTM\n",
    "- комбинируем конкатенацией\n",
    "\n",
    "![GraphSAGE](img/09.png)\n",
    "\n",
    "\n",
    "### 100500 производных - GAT, GIN и т.д.\n",
    "\n",
    "### GNN\n",
    "\n",
    "На мой взгляд объяснение GNN, которое дано в статье, просто обобщает все вышесказанное.\n",
    "\n",
    "![GNN](img/07.png)\n",
    "\n",
    "В базовой реализиации, алгоритм инциализируется случайным эмбеддингом для каждой ноды и на каждой итерации эмбеддинги накапливают информацию о своих соседях вот таким образом:\n",
    "\n",
    "![GNN embeddings](img/08.png)\n",
    "\n",
    "где вот эта $h$ - это произвольная дифференцируемая функция вида $рЖ \\mathbb{R}^d \\times \\mathbb{R}^m \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}^d$. Чтобы это не означало, на выходе после $K$ итераций мы должны получить вектор вида $\\mathbf{z}_{v_i} = g({\\mathbf{h}_i}^K)$, где $g$ - произвольня дифференцируемая функция вида $g: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, ну в общем ээто некая нейроночка, может быть MLP или какая-то другая конструкция. В статье приводится несколько ссылок на работы, посвященные этой тематике.\n",
    "\n",
    "В лекции №8, после краткого рассказа с картинками, мы сразу перешли к объяснению GCN из-за чего произошла путаница.\n",
    "\n",
    "![HNN-GCN](img/10.png)\n",
    "![HNN-GCN](img/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* Что там еще в статье? \n",
    "- мультимодальные графы (что делать с разными нодами и ребрами)\n",
    "- структурные роли нод в графе\n",
    "- эмбеддинги субграфов\n",
    "\n",
    "[Representation Learning on Graphs: Methods and Applications](https://arxiv.org/abs/1709.05584)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
